# The events Package

## Installation

When we get back on the CRAN wagon, you can install this package with
```
install.packages("events")
```
Until that day, you can install directly from Github using the `remotes`
package, which you'll want to install first. Then
```
remotes::install_github("events")
```

## Purpose

The aim of events is to make life a bit easier for people who
analyse event data: the kind of thing that KEDS/TABARI generates as
output (see e.g. from the Parus Analytics homepages).  
There's nothing fancy in the package, just a hopefully logical
interface to all the data massaging we do to event data
before any actual analysis.

The philosophy of the package is that you will want to read in raw
event data and then apply a sequence of filters and aggregators for
actors and event types, each of which will result in an new event data set.
The final step is an application of `make_dyads` to make a set of named 
temporally regular directed dyad time series suitable for time 
series analysis. It's all rather unix-ish, if you care for that 
sort of thing.

There's a fairly complete walkthrough in the package vignette.  You
should probably read that.

The package now contains several more classic event data sets as data.
These were originally generated by Phil Schrodt, and all questions 
about their substantive content should be directed to him. I am 
only making them available to you with his permission. You can find even more event data from the same source on his DataVerse pages.

## Caveat

This package was written before the tidyverse was in full swing. Consequently
almost every function here can be done better and faster using functions 
from `dplyr` and `tidyr`. Or doubtless even faster using `data.table`

If you are a `data.table` or `tidyverse`-loving person (like the package maintainer) then the most useful thing about `events` is that it 
bundles some classic KEDS/TABARI event data sets.

## Precursors

The package is ultimately intended to unify the existing software,
e.g.  the packages currently linked from the PSU event
data pages. (As of 30.12.2021 these pages are not available either)

The unification is certainly not complete.
In particular, extremely large data sets are probably going to be
rather unwieldy in the current version.

The sections below provide a quick compare and contrast to the 
software formerly available from Penn State and now from Parus 
Anaytics, that work with event data output 
(not Factiva stuff or actor dictionaries).

### scrubkeds

This functionality is implemented as a standalone function
`scrub_keds` and as an option in the event data reading function
`read_keds`.

### One-A-Day_Filter

This is also a standalone function `one_a_day` and available as an
option in the event data reading function `read_keds`.

### The KEDS_Count standalone program

Most, but not all, of the functionality of `KEDS_Count` is implemented.  
Some differences are noted below:

 * Aggregation is possible by day, week, month, quarter and year, but
   not biweekly.

 * `read_keds` assumes that two digit year specifications 69 to 99
   indicate years between 1969 to 1999, and 00 to 68 indicate 2000 to 2068.  
   This is R and the POSIX standard interpretation for this needlessly
   ambiguous date formulation.  In contrast the `KEDS_Count` 
   program treats 28 to 99 as 
   in the twentieth century and 00 to 27 as in the twenty first century.

 * Wildcarding for actors and event codes is not implemented
   directly, so you have to use R's facilities.  An example using
   `grep` is provided in the vignette.  Also, using "***" to mean 'all
   targets' is done by using `filter_actors` with a suitable second
   parameter.

 * `KEDS_Count` aggregates up to a temporal unit, e.g. a week,
   differently to `events`:  It tosses events that occur before the
   beginning of the first full unit.  `make_dyads` does not.
   There is no warning about this behaviour, but then it doesn't eat
   your data either.

 * `KEDS_Count` only offers summed scores and provides the per time unit
   N to construct means.  This package will use any aggregating function
   you give it, e.g. `sd` or `median` and also automatically generate
   N.
   
### Aggregator

Unlike `Aggregator`, events does not have a customizable aggregation
period, but it is a lot more flexible about what the aggregation
function is (see above).

### Event_Filter

I can't figure out what this program did, so there are probably no
functions to replicate it.


